{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "verbal-travel",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "## Instructions\n",
    "\n",
    "The goal of the challenge is to classify a tabular dataset with *1024 input features* and separated in *10 classes*. The train and test set are provided.\n",
    "\n",
    "The classification score is computed on the test set using the Categorical Cross Entropy loss. An implementation is provided in the `classification_score` function. Of course this function should be used after the model training finished!\n",
    "\n",
    "The best score in the litterature for this dataset is **Loss = 0.15** and **Accuracy = 0.95**. \n",
    "\n",
    "The objective is to qualify your methodology, expertise and creativity.\n",
    "\n",
    "You will present your results within the jupyter notebook.\n",
    "\n",
    "Your code should be written in **Pytorch**.\n",
    "\n",
    "Tips: Tabular data are unstructured by definition. Finding the underlying structure will strongly enhance your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-strain",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "protective-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "neutral-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, model):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(50):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(\"Training loss \",epoch,\": \",running_loss/len(trainloader))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def save(model, model_path):\n",
    "    \n",
    "    torch.save(model, model_path)\n",
    "        \n",
    "def get_train_accuracy(model, trainloader):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total \n",
    "\n",
    "def get_test_accuracy(model, testloader):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "def classification_score(model, x_test, y_test):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    x_test = x_test.to(device) \n",
    "    y_test = y_test.to(device)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model(x_test)\n",
    "\n",
    "    # Calculate loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(y_pred, y_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = (y_pred.argmax(dim=1) == y_test).float().mean()\n",
    "\n",
    "    print('Loss: {:.3f}, Accuracy: {:.3f}'.format(loss.item(), acc))\n",
    "\n",
    "    return loss.item(), acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f37d1e",
   "metadata": {},
   "source": [
    "## PyTorch Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "752196de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=1024, num_classes=10):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, 512) \n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.layer2 = nn.Linear(512, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.layer4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.dropout4 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.layer5 = nn.Linear(32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.layer1(x)) \n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = F.softmax(self.layer5(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a8f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(2, 2) \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1) \n",
    "        self.pool2 = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x) \n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aerial-desktop",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CNNPermInvTrial(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(CNNPermInvTrial, self).__init__()\n",
    "       \n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.gap(x)\n",
    "        \n",
    "        x = x.view(-1, 32)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-burst",
   "metadata": {},
   "source": [
    "## Experiments Part 1: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f72d4",
   "metadata": {},
   "source": [
    "**MLP MODEL - DATASET AND MODEL PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ancient-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('X_train.npy')\n",
    "X_train = np.reshape(x_train, (50000, 1024))\n",
    "y_train = np.load('Y_train.npy')\n",
    "y_train = np.reshape(y_train, (50000, ))\n",
    "x_test = np.load('X_test.npy')\n",
    "X_test = np.reshape(x_test, (10000, 1024))\n",
    "y_test = np.load('Y_test.npy')\n",
    "y_test = np.reshape(y_test, (10000, ))\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long() \n",
    "    \n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "model = MLP() \n",
    "model = torch.load('Models/original_50.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c739c0",
   "metadata": {},
   "source": [
    "**MLP MODEL - EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fd2bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Accuracy:  0.32166 \n",
      "Test Accuracy:  0.3134\n"
     ]
    }
   ],
   "source": [
    "train_acc = get_train_accuracy(model, trainloader)\n",
    "test_acc = get_test_accuracy(model, testloader)\n",
    "print('\\nTrain Accuracy: ',train_acc,'\\nTest Accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d2c7a",
   "metadata": {},
   "source": [
    "**CNN MODEL - DATASET AND MODEL PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b6e9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('X_train.npy')\n",
    "X_train = np.reshape(x_train, (50000, 32, 32))\n",
    "y_train = np.load('Y_train.npy')\n",
    "y_train = np.reshape(y_train, (50000, ))\n",
    "x_test = np.load('X_test.npy')\n",
    "X_test = np.reshape(x_test, (10000, 32, 32))\n",
    "y_test = np.load('Y_test.npy')\n",
    "y_test = np.reshape(y_test, (10000, ))\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long() \n",
    "    \n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "model_100epoch, model_250epoch = CNN(), CNN()\n",
    "model_100epoch = torch.load('Models/collective_100.pth')\n",
    "model_250epoch = torch.load('Models/collective_250.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36699523",
   "metadata": {},
   "source": [
    "**CNN MODEL - EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef169891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Accuracy - 100 Epochs:  0.35866 \n",
      "Test Accuracy - 100 Epochs:  0.2735\n",
      "\n",
      "Train Accuracy - 250 Epochs:  0.4528 \n",
      "Test Accuracy - 250 Epochs:  0.3179\n"
     ]
    }
   ],
   "source": [
    "train_acc1 = get_train_accuracy(model_100epoch, trainloader)\n",
    "test_acc1 = get_test_accuracy(model_100epoch, testloader)\n",
    "print('\\nTrain Accuracy - 100 Epochs: ',train_acc1,'\\nTest Accuracy - 100 Epochs: ',test_acc1)\n",
    "\n",
    "train_acc2 = get_train_accuracy(model_250epoch, trainloader)\n",
    "test_acc2 = get_test_accuracy(model_250epoch, testloader)\n",
    "print('\\nTrain Accuracy - 250 Epochs: ',train_acc2,'\\nTest Accuracy - 250 Epochs: ',test_acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5320650",
   "metadata": {},
   "source": [
    "**CNN (GLOBAL AVERAGE POOLING) - TRAINING DEMONSTRATED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f4bd8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss  0 :  2.177089918369066\n",
      "Training loss  1 :  2.132926947172071\n",
      "Training loss  2 :  2.126747092831539\n",
      "Training loss  3 :  2.1203662581117353\n",
      "Training loss  4 :  2.1056951384321665\n",
      "Training loss  5 :  2.0836199610491097\n",
      "Training loss  6 :  2.0721624943970567\n",
      "Training loss  7 :  2.066424259373719\n",
      "Training loss  8 :  2.0622528930238175\n",
      "Training loss  9 :  2.058015494032388\n",
      "Training loss  10 :  2.0535742544014335\n",
      "Training loss  11 :  2.048881127829744\n",
      "Training loss  12 :  2.043821997087275\n",
      "Training loss  13 :  2.038518985265047\n",
      "Training loss  14 :  2.0328484679824332\n",
      "Training loss  15 :  2.026864065166017\n",
      "Training loss  16 :  2.020539047850757\n",
      "Training loss  17 :  2.014266214001583\n",
      "Training loss  18 :  2.0076106168181944\n",
      "Training loss  19 :  2.0010501175298954\n",
      "Training loss  20 :  1.9941367603461864\n",
      "Training loss  21 :  1.9873234507981128\n",
      "Training loss  22 :  1.9804210754365243\n",
      "Training loss  23 :  1.9738631937188058\n",
      "Training loss  24 :  1.9669915147874115\n",
      "Training loss  25 :  1.9602616815634133\n",
      "Training loss  26 :  1.9534578869454158\n",
      "Training loss  27 :  1.9466767659647986\n",
      "Training loss  28 :  1.939782807175654\n",
      "Training loss  29 :  1.9328519266077286\n",
      "Training loss  30 :  1.9261840028939763\n",
      "Training loss  31 :  1.9199235330235096\n",
      "Training loss  32 :  1.9137201765295946\n",
      "Training loss  33 :  1.9078613780510403\n",
      "Training loss  34 :  1.9025181305385597\n",
      "Training loss  35 :  1.8975068030262787\n",
      "Training loss  36 :  1.8925925339175889\n",
      "Training loss  37 :  1.8882944966003212\n",
      "Training loss  38 :  1.8840678538257163\n",
      "Training loss  39 :  1.8799993088820464\n",
      "Training loss  40 :  1.8762616909854473\n",
      "Training loss  41 :  1.8730105346239154\n",
      "Training loss  42 :  1.8694026693654076\n",
      "Training loss  43 :  1.8663215178827102\n",
      "Training loss  44 :  1.8631480987149787\n",
      "Training loss  45 :  1.8603538498234764\n",
      "Training loss  46 :  1.8570391272247715\n",
      "Training loss  47 :  1.854032451802923\n",
      "Training loss  48 :  1.851247111880009\n",
      "Training loss  49 :  1.8483434393858955\n"
     ]
    }
   ],
   "source": [
    "model = CNNPermInvTrial()\n",
    "train(trainloader, model)\n",
    "save(model, 'Models/perminv_50.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ef3d4",
   "metadata": {},
   "source": [
    "**CNN (GLOBAL AVERAGE POOLING) - MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd3ee70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Accuracy:  0.33 \n",
      "Test Accuracy:  0.3238\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('Models/perminv_50.pth')\n",
    "train_acc = get_train_accuracy(model, trainloader)\n",
    "test_acc = get_test_accuracy(model, testloader)\n",
    "print('\\nTrain Accuracy: ',train_acc,'\\nTest Accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b437b0d",
   "metadata": {},
   "source": [
    "## Experiments Part 2: Failed but Different Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1e975",
   "metadata": {},
   "source": [
    "**DATA EXPLORATION AND PREPARATION UTILITY FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6ea23e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def standardize_dataset(dataset, mean=None, std=None):\n",
    "    \n",
    "    # Calculate mean and standard deviation across the entire dataset\n",
    "    if mean==None and std==None:\n",
    "        mean = np.mean(dataset)\n",
    "        std = np.std(dataset)\n",
    "    \n",
    "    # Standardize the dataset\n",
    "    standardized_dataset = (dataset - mean) / std\n",
    "    \n",
    "    return standardized_dataset, mean, std\n",
    "\n",
    "\n",
    "def reassign(x_train):\n",
    "    \n",
    "    x_train[(x_train < -1.0)] = -2.0\n",
    "    x_train[(x_train > -1.0) & (x_train < 1)] = 0.0\n",
    "    x_train[(x_train > 1)] = 2.0\n",
    "    \n",
    "    return x_train\n",
    "\n",
    "def plot(data, index):\n",
    "    \n",
    "    plt.imshow(data[index], cmap='gray')  \n",
    "    plt.axis('off')  \n",
    "    plt.show()\n",
    "    \n",
    "def plot_single(image):\n",
    "    \n",
    "    plt.imshow(image, cmap='gray')  \n",
    "    plt.axis('off')  \n",
    "    plt.show()\n",
    "    \n",
    "def get_separate_classes(data, target):\n",
    "    \n",
    "    max_element = target.max()\n",
    "    dict_ind = {i:[] for i in range(max_element+1)}\n",
    "    for i in range(max_element+1):\n",
    "        indices = np.where(target == i)[0]\n",
    "        dict_ind[i] = data[indices]\n",
    "        \n",
    "    return dict_ind\n",
    "\n",
    "\n",
    "def plot_sequence_distribution(dataset, num_buckets=10):\n",
    "    \n",
    "    # Determine the range of values across all sequences\n",
    "    min_value = min(min(seq) for seq in dataset)\n",
    "    max_value = max(max(seq) for seq in dataset)\n",
    "    \n",
    "    # Define bucket edges\n",
    "    bucket_edges = np.linspace(min_value, max_value, num_buckets + 1)\n",
    "    \n",
    "    # Initialize histogram counts for each bucket\n",
    "    hist_counts = np.zeros((len(dataset), num_buckets))\n",
    "    \n",
    "    # Count values falling into each bucket for each sequence\n",
    "    for i, seq in enumerate(dataset):\n",
    "        hist, _ = np.histogram(seq, bins=bucket_edges)\n",
    "        hist_counts[i] = hist\n",
    "    \n",
    "    # Plot distribution of sequence values\n",
    "    for hist in hist_counts:\n",
    "        plt.plot(bucket_edges[:-1], hist, marker='o')\n",
    "    \n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Sequence Values')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def image_to_graph(image):\n",
    "    \n",
    "    # Create an empty graph\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height, width = image.shape\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            node_id = i * width + j\n",
    "            graph.add_node(node_id, pixel_value=image[i, j])\n",
    "    \n",
    "    # Add edges between adjacent pixels\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            current_node_id = i * width + j\n",
    "            # Check neighboring pixels\n",
    "            for ni in range(i - 1, i + 2):\n",
    "                for nj in range(j - 1, j + 2):\n",
    "                    # Ensure neighbor is within image boundaries and not the same as the current pixel\n",
    "                    if 0 <= ni < height and 0 <= nj < width and (ni != i or nj != j):\n",
    "                        neighbor_node_id = ni * width + nj\n",
    "                        # Calculate edge weight as absolute difference between pixel values\n",
    "                        edge_weight = abs(image[i, j] - image[ni, nj])\n",
    "                        # Add edge to the graph\n",
    "                        graph.add_edge(current_node_id, neighbor_node_id, weight=edge_weight)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def display_graph(graph):\n",
    "    \n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    pos = nx.spring_layout(graph)  # Position nodes using the spring layout algorithm\n",
    "    nx.draw(graph, pos, with_labels=False, node_size=10, edge_color='gray', alpha=0.7)\n",
    "    \n",
    "    # Display edge weights\n",
    "    labels = nx.get_edge_attributes(graph, 'weight')\n",
    "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=labels)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.title('Graph Representation of Image')\n",
    "    plt.show()    \n",
    "    \n",
    "def select_coordinates(image, c_start, c_end, c_size, threshold):\n",
    "    \n",
    "    # Get coordinates of pixels with values less than the threshold\n",
    "    filtered_indices = np.where(image < threshold)\n",
    "    filtered_x, filtered_y = filtered_indices\n",
    "    \n",
    "    # Randomly select c_size coordinates from the filtered coordinates\n",
    "    selected_indices = np.random.choice(len(filtered_x), size=c_size, replace=False)\n",
    "    selected_x = filtered_x[selected_indices]\n",
    "    selected_y = filtered_y[selected_indices]\n",
    "    \n",
    "    return selected_x, selected_y\n",
    "\n",
    "\n",
    "def local_permute_pixels(image):\n",
    "\n",
    "    img_height, img_width = image.shape[:2]\n",
    "  \n",
    "    c_size = 20 #img_height//2\n",
    "    \n",
    "    c_start = 6 #img_height//2 - c_size//2\n",
    "    c_end = 26   #c_start + c_size\n",
    "  \n",
    "    b_top = 0\n",
    "    b_bottom = img_height\n",
    "    b_left = 0\n",
    "    b_right = img_width\n",
    "    \n",
    "    #central_x_coordinates, central_y_coordinates = select_coordinates(image, c_start, c_end, c_size, 0)\n",
    "\n",
    "    central_x_coordinates = np.random.choice(np.arange(c_start, c_end), size=c_size, replace=False)\n",
    "    central_y_coordinates = np.random.choice(np.arange(c_start, c_end), size=c_size, replace=False)\n",
    "    \n",
    "    leftperi_x_coordinates = np.random.choice(np.arange(b_left, c_start), size=c_size//4, replace=False)\n",
    "    leftperi_y_coordinates = np.random.choice(np.arange(b_top, b_bottom), size=c_size//4, replace=False)\n",
    "    \n",
    "    rightperi_x_coordinates = np.random.choice(np.arange(c_end, b_right), size=c_size//4, replace=False)\n",
    "    rightperi_y_coordinates = np.random.choice(np.arange(b_top, b_bottom), size=c_size//4, replace=False)\n",
    "    \n",
    "    topperi_x_coordinates = np.random.choice(np.arange(b_left, b_right), size=c_size//4, replace=False)\n",
    "    topperi_y_coordinates = np.random.choice(np.arange(b_top, c_start), size=c_size//4, replace=False)\n",
    "    \n",
    "    bottomperi_x_coordinates = np.random.choice(np.arange(b_left, b_right), size=c_size//4, replace=False)\n",
    "    bottomperi_y_coordinates = np.random.choice(np.arange(c_end, b_bottom), size=c_size//4, replace=False)\n",
    "    \n",
    "    central = np.array([central_x_coordinates, central_y_coordinates]).T\n",
    "    \n",
    "    left_peripheral = np.array([leftperi_x_coordinates, leftperi_y_coordinates]).T\n",
    "    right_peripheral = np.array([rightperi_x_coordinates, rightperi_y_coordinates]).T\n",
    "    top_peripheral = np.array([topperi_x_coordinates, topperi_y_coordinates]).T\n",
    "    bottom_peripheral = np.array([bottomperi_x_coordinates, bottomperi_y_coordinates]).T\n",
    "    \n",
    "    peripheral_stacked = np.stack((left_peripheral, right_peripheral, top_peripheral, bottom_peripheral),axis=0)\n",
    "    peripheral = np.reshape(peripheral_stacked, (c_size, 2))\n",
    "    \n",
    "    return central, peripheral\n",
    "\n",
    "\n",
    "def get_new_image(image, central, peripheral):\n",
    "    \n",
    "    img_height, img_width = image.shape[:2]\n",
    "  \n",
    "    c_size = 20 #img_height//2\n",
    "    \n",
    "    c_start = 6 #img_height//2 - c_size//2\n",
    "    c_end = 26   #c_start + c_size\n",
    "  \n",
    "    new_image = copy.deepcopy(image)\n",
    "    new_image[peripheral] = image[central]\n",
    "    new_image[c_start:c_end, c_start:c_end] = 0.0\n",
    "\n",
    "    return new_image\n",
    "\n",
    "def assign_pixel_values_closest_match(image, central_coords, peripheral_coords):\n",
    "    \n",
    "    img_height, img_width = image.shape[:2]\n",
    "  \n",
    "    c_size = 20 #img_height//2\n",
    "    \n",
    "    c_start = 6 #img_height//2 - c_size//2\n",
    "    c_end = 26   #c_start + c_size\n",
    "    \n",
    "    new_image = np.copy(image)  # Create a copy of the peripheral image\n",
    "    \n",
    "    for p_coord in peripheral_coords:\n",
    "        min_distance = float('inf')\n",
    "        closest_c_coord = None\n",
    "        \n",
    "        # Calculate Euclidean distance between the peripheral coordinate and all central coordinates\n",
    "        for c_coord in central_coords:\n",
    "            distance = np.linalg.norm(p_coord - c_coord)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_c_coord = c_coord\n",
    "        \n",
    "        # Assign pixel value from the closest central coordinate to the peripheral coordinate\n",
    "        p_x, p_y = p_coord\n",
    "        c_x, c_y = closest_c_coord\n",
    "        new_image[p_x, p_y] = image[c_x, c_y]\n",
    "        new_image[c_start:c_end, c_start:c_end] = 0\n",
    "    \n",
    "    return new_image\n",
    "\n",
    "def perturb_dataset(original_X):\n",
    "\n",
    "    X_train_smallprt = {i:[] for i in range(10)}\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        unperturbed_matrices = original_X[i]\n",
    "        perturbed_matrices = []\n",
    "\n",
    "        for image_no in range(unperturbed_matrices.shape[0]):\n",
    "\n",
    "            img = unperturbed_matrices[image_no]\n",
    "            new_img = get_new_image(img, central, peripheral)\n",
    "            perturbed_matrices.append(new_img)\n",
    "\n",
    "        X_train_smallprt[i] = np.array(perturbed_matrices)\n",
    "        \n",
    "    return X_train_smallprt\n",
    "\n",
    "\n",
    "def prepare_dataset_for_CNN(X_separated):\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for key in range(10):\n",
    "        \n",
    "        for image_no in range(X_separated[key].shape[0]):\n",
    "            \n",
    "            img = np.reshape(X_separated[key][image_no], (1024,))\n",
    "            X_list.append(img)\n",
    "            y_list.append(key)\n",
    "        \n",
    "    X_arr = np.array(X_list)\n",
    "    y_arr = np.array(y_list)\n",
    "    \n",
    "    total_num = len(y_list)\n",
    "    X = np.reshape(X_arr, (total_num, 32, 32))\n",
    "    y = np.reshape(y_arr, (total_num,))\n",
    "    \n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43348dd9",
   "metadata": {},
   "source": [
    "**STANDARDIZED AND PERTURBED DATA LOADING AND BEST BASELINE MODEL PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "008407e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train_smallprt0.npy')\n",
    "X_test = np.load('X_test_smallprt0.npy')\n",
    "y_train = np.load('y_train_smallprt0.npy')\n",
    "y_test = np.load('y_test_smallprt0.npy')\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long() \n",
    "    \n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "model = CNN()\n",
    "model = torch.load('Models/collective_250.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cddeab8",
   "metadata": {},
   "source": [
    "**CNN 250 EPOCH - MODEL EVALUATION ON PERTURBED DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71b7ab18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Accuracy:  0.4582 \n",
      "Test Accuracy:  0.3179\n"
     ]
    }
   ],
   "source": [
    "train_acc = get_train_accuracy(model, trainloader)\n",
    "test_acc = get_test_accuracy(model, testloader)\n",
    "print('\\nTrain Accuracy: ',train_acc,'\\nTest Accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37996fe8",
   "metadata": {},
   "source": [
    "**DATA AND MODEL LOADING FOR MoE TRIAL (MIXTURE OF EXPERTS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd9a3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('X_train.npy')\n",
    "X_train = np.reshape(x_train, (50000, 32, 32))\n",
    "y_train = np.load('Y_train.npy')\n",
    "y_train = np.reshape(y_train, (50000, ))\n",
    "x_test = np.load('X_test.npy')\n",
    "X_test = np.reshape(x_test, (10000, 32, 32))\n",
    "y_test = np.load('Y_test.npy')\n",
    "y_test = np.reshape(y_test, (10000, ))\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long() \n",
    "    \n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "loaded_models = []\n",
    "for m in range(10):\n",
    "    \n",
    "    model = CNN()\n",
    "    mname = 'Models/m'+str(m)+'.npy'\n",
    "    model.load_state_dict(torch.load(mname))\n",
    "    loaded_models.append(model)\n",
    "    \n",
    "def classify_image(image, model):\n",
    "    \n",
    "    # Convert the image to torch tensor and add batch dimension\n",
    "    image = torch.tensor(image).unsqueeze(0)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        output = model(image)\n",
    "        starkness = torch.max(output) - torch.min(output)\n",
    "        max_logit = torch.max(output)\n",
    "        idx = torch.where(output == max_logit)[1]\n",
    "        predicted_label = idx[0]\n",
    "        \n",
    "    return starkness, max_logit, predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f938cb",
   "metadata": {},
   "source": [
    "**MoE - EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c2a72cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/wc3bz6qs53q1xnsk8pkm329c0000gn/T/ipykernel_69307/3961904915.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image).unsqueeze(0)\n",
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy %:  10.0\n",
      "Testing Accuracy %:  10.03\n"
     ]
    }
   ],
   "source": [
    "#training evaluation\n",
    "train_length = X_train.shape[0]\n",
    "train_correct = 0\n",
    "for image_no in range(train_length):\n",
    "    \n",
    "    image = X_train[image_no]\n",
    "    actual_label = y_train[image_no]\n",
    "    \n",
    "    logits = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    for model in loaded_models:\n",
    "        \n",
    "        _, max_logit, pred = classify_image(image, model)\n",
    "        stk_item = max_logit.item()\n",
    "        pred_item = pred.item()\n",
    "        \n",
    "        logits.append(stk_item)\n",
    "        predicted_labels.append(pred_item)\n",
    "        \n",
    "    most_certain_model = logits.index(max(logits))\n",
    "    chosen_class = predicted_labels[most_certain_model]\n",
    "    \n",
    "    if int(chosen_class) == int(actual_label):\n",
    "        \n",
    "        train_correct += 1\n",
    "        \n",
    "        \n",
    "print('Training Accuracy %: ', (train_correct/train_length)*100)    \n",
    "\n",
    "# testing evaluation\n",
    "test_length = X_test.shape[0]\n",
    "test_correct = 0\n",
    "for image_no in range(test_length):\n",
    "    \n",
    "    image = X_test[image_no]\n",
    "    actual_label = y_test[image_no]\n",
    "    \n",
    "    starkness = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    for model in loaded_models:\n",
    "        \n",
    "        stk, _, pred = classify_image(image, model)\n",
    "        stk_item = stk.item()\n",
    "        pred_item = pred.item()\n",
    "        \n",
    "        starkness.append(stk_item)\n",
    "        predicted_labels.append(pred_item)\n",
    "        \n",
    "    most_certain_model = starkness.index(max(starkness))\n",
    "    chosen_class = predicted_labels[most_certain_model]\n",
    "    \n",
    "    if chosen_class == actual_label:\n",
    "        \n",
    "        test_correct += 1        \n",
    "        \n",
    "print('Testing Accuracy %: ', (test_correct/test_length)*100) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f07c54",
   "metadata": {},
   "source": [
    "## Thoughts behind attempted approaches and what might've worked better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c48e58a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.733, Accuracy: 0.318\n"
     ]
    }
   ],
   "source": [
    "#Summarising best model so far (which, tbh, seriously needs improvment)\n",
    "\n",
    "x_train = np.load('X_train.npy')\n",
    "X_train = np.reshape(x_train, (50000, 32, 32))\n",
    "y_train = np.load('Y_train.npy')\n",
    "y_train = np.reshape(y_train, (50000, ))\n",
    "x_test = np.load('X_test.npy')\n",
    "X_test = np.reshape(x_test, (10000, 32, 32))\n",
    "y_test = np.load('Y_test.npy')\n",
    "y_test = np.reshape(y_test, (10000, ))\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long() \n",
    "\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "model = CNN()\n",
    "model = torch.load('Models/collective_250.pth')\n",
    "loss, acc = classification_score(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd319c64",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "In these series of approaches, I have not put my focus on the neurel network architecture, but rather focused on pre-processing of the data, exploring the effect of permutation and the failure of pooling functions (both max and average) due to the image pixel scrambling. And i've also focus one approach on attempting a mixture of experts system, which failed. \n",
    "\n",
    "**Approaches from Part I -** \n",
    "\n",
    "The best approach from Part I was from a simple CNN model trained over data restructured into images. it performed better than the MLP model - translated from the TF code to PyTorch code, and surprisingly, better than than the CNN model with global average pooling which I believed would yield better results due to increased ability of maintaining permutation equivariance. \n",
    "\n",
    "Loss and Accuracy of the best model - \n",
    "\n",
    "    Loss: 11.733\n",
    "    Accuracy: 0.318\n",
    "\n",
    "**Approaches from Part II -** \n",
    "\n",
    "The first approach from part II was a data pre-processing approach where the images were extremized to bring more discrete sections of sub-images. The nest part of the approach was perturbing some of the pixel values in such a way that at least one perturbation may bring the images closer to the original unciphered images. In retrospect, I doubt this was a good use of time since the whole point of the cipher was to protect the data and classify without unscrambling it.  \n",
    "\n",
    "The second approach was to train ten models (for each class) on their own corresponding set of images with a few added from the other nine classes (this is also more computationally efficient). And each of these models are treated as experts in their class. Then during evaluation, these ten models are each given a vote on which class a specific image belongs to, and the prediction with the maximum logit-based confidence is chosen. This method also failed with an unacceptably low accuracy. \n",
    "\n",
    "**WHAT WAS LEARNT...**\n",
    "\n",
    "Thinking about this from first-principles - when an image has it's pixels permuted, the aggregate high-level patterns disappear, and what remains constant is the overall distribution of the pixels in the image. Now if the permutation is the same across all images, then per sub-block of image, there should still be similar sub-distributions of pixel values. However, when a closer look was taken at the images, there was no tangible similarity in many of these images. And upon reading a bit about the original CIFAR data, this is my conclusion; the objects in the pictures are in quite different orientations, different levels of zoomification, and different contrast settings. While I haven't thought about a way to get around the orientation and zoomification problem yet, the third part - contrast settings perhaps be worked with. If all images were normalised to a low contrast standardised between -0.5 and 0.5 (-1 and 1, whichever yields better) and then two models were trained - one on this described tranform and another on the inverse, and then a higher confidenec score was selected, it may yield better results (but then again it may not, from last seven hour's experience). \n",
    "\n",
    "**PAST OPEN-SOURCE IMPLEMENTED RESEARCH I WOULD ATTEMPT IF TIME PERMITTED...**\n",
    "\n",
    "1. https://openreview.net/pdf?id=eL1iX7DMnPI \n",
    "2. https://arxiv.org/pdf/2001.07761.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07a6c1",
   "metadata": {},
   "source": [
    "## IMPLEMENTATION IN TF KERAS\n",
    "**This was not simulated on my system, since I ran into the problem of my Jupyter kernel crashing each time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "useful-tribune",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Exemple in Keras\\nfrom tensorflow.keras.models import Sequential \\nfrom tensorflow.keras import layers \\nfrom tensorflow.keras.callbacks import EarlyStopping \\n\\ndef simple_mlp_exemple_in_keras(x_train, y_train):\\n    \\n    model = Sequential() \\n    model.add(layers.Dense(512, kernel_initializer = \\'normal\\', activation = \\'relu\\', input_shape = (1024,))) \\n    model.add(layers.BatchNormalization())\\n    model.add(layers.Dropout(0.2))\\n    model.add(layers.Dense(128, activation = \\'relu\\'))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.Dropout(0.2))\\n    model.add(layers.Dense(64, activation = \\'relu\\'))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.Dropout(0.2))\\n    model.add(layers.Dense(32, activation = \\'relu\\'))\\n    model.add(layers.BatchNormalization())\\n    model.add(layers.Dropout(0.2))\\n    model.add(layers.Dense(10, activation = \\'softmax\\'))\\n\\n    model.compile(\\n       loss = \\'categorical_crossentropy\\', \\n       optimizer = \"adam\", \\n       metrics = [\\'accuracy\\']\\n    )\\n    \\n    es = EarlyStopping(monitor=\\'val_loss\\', mode=\\'min\\', verbose=1, patience=5)\\n    \\n    history = model.fit(\\n       x_train, y_train,    \\n       batch_size=128, \\n       epochs = 50, \\n       verbose = 1, \\n       validation_split = 0.2,\\n       callbacks=[es]\\n    )\\n    return model\\n\\nmodel = simple_mlp_exemple_in_keras(x_train, y_train)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple in Keras\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "def simple_mlp_exemple_in_keras(x_train, y_train):\n",
    "    \n",
    "    model = Sequential() \n",
    "    model.add(layers.Dense(512, kernel_initializer = 'normal', activation = 'relu', input_shape = (1024,))) \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(128, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(32, activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "    model.compile(\n",
    "       loss = 'categorical_crossentropy', \n",
    "       optimizer = \"adam\", \n",
    "       metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    \n",
    "    history = model.fit(\n",
    "       x_train, y_train,    \n",
    "       batch_size=128, \n",
    "       epochs = 50, \n",
    "       verbose = 1, \n",
    "       validation_split = 0.2,\n",
    "       callbacks=[es]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = simple_mlp_exemple_in_keras(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-annex",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "animated-overall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tensorflow.keras.losses import CategoricalCrossentropy\\nfrom tensorflow.keras.metrics import CategoricalAccuracy\\n\\ndef classification_score(model, ohe, scaler):\\n    \\n    Categorical Cross Entropy Score function\\n    \\n        Parameters\\n        ----------\\n        model : sklearn-like class of the classification model.\\n                It requires a predict method.\\n        ohe : One Hot Encoder model for label encoding \\n        scaler : Normalisation function\\n  \\n    # Load data\\n    x_test = np.load('X_test.npy')\\n    y_test = np.load('Y_test.npy')\\n    \\n    # Label Encoder\\n    y_test = ohe.transform(y_test).toarray()\\n    \\n    # Samples Normalisation\\n    x_test = scaler.transform(x_test)\\n    \\n    # Predict\\n    y_pred = model.predict(x_test)\\n    \\n    # Score\\n    cce = CategoricalCrossentropy()\\n    ac = CategoricalAccuracy()\\n    \\n    loss = cce(y_test, y_pred).numpy()\\n    acc = ac(y_test, y_pred).numpy()\\n    \\n    print('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\\n    return (loss, acc)\\n\\n(loss, acc) = classification_score(model, ohe, scaler)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "\n",
    "def classification_score(model, ohe, scaler):\n",
    "    \n",
    "    Categorical Cross Entropy Score function\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : sklearn-like class of the classification model.\n",
    "                It requires a predict method.\n",
    "        ohe : One Hot Encoder model for label encoding \n",
    "        scaler : Normalisation function\n",
    "  \n",
    "    # Load data\n",
    "    x_test = np.load('X_test.npy')\n",
    "    y_test = np.load('Y_test.npy')\n",
    "    \n",
    "    # Label Encoder\n",
    "    y_test = ohe.transform(y_test).toarray()\n",
    "    \n",
    "    # Samples Normalisation\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Score\n",
    "    cce = CategoricalCrossentropy()\n",
    "    ac = CategoricalAccuracy()\n",
    "    \n",
    "    loss = cce(y_test, y_pred).numpy()\n",
    "    acc = ac(y_test, y_pred).numpy()\n",
    "    \n",
    "    print('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n",
    "    return (loss, acc)\n",
    "\n",
    "(loss, acc) = classification_score(model, ohe, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e35c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
